{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-18T15:47:36.265144Z",
     "start_time": "2024-11-18T15:46:05.725662Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def display_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(f\"Current memory usage: {process.memory_info().rss / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "def process_chunk(chunk, label_encoders=None, first_chunk=False):\n",
    "    \"\"\"Process a single chunk of data\"\"\"\n",
    "    if first_chunk:\n",
    "        label_encoders = {}\n",
    "        categorical_columns = chunk.select_dtypes(include=['object']).columns\n",
    "        for col in categorical_columns:\n",
    "            label_encoders[col] = LabelEncoder()\n",
    "            chunk[col] = label_encoders[col].fit_transform(chunk[col].astype(str))\n",
    "    else:\n",
    "        categorical_columns = label_encoders.keys()\n",
    "        for col in categorical_columns:\n",
    "            chunk[col] = chunk[col].astype(str)\n",
    "            chunk[col] = label_encoders[col].transform(chunk[col])\n",
    "\n",
    "    # Convert to float32 for numeric columns\n",
    "    numeric_columns = chunk.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_columns:\n",
    "        chunk[col] = chunk[col].astype(np.float64)\n",
    "\n",
    "    return chunk, label_encoders\n",
    "\n",
    "try:\n",
    "    print(\"Starting analysis...\")\n",
    "    display_memory_usage()\n",
    "\n",
    "    # Create directories\n",
    "    os.makedirs('Output', exist_ok=True)\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "\n",
    "    # Initialize variables\n",
    "    chunk_size = 1000\n",
    "    label_encoders = None\n",
    "    first_chunk = True\n",
    "    feature_names = None\n",
    "\n",
    "    # Identify unique classes for the target variable\n",
    "    print(\"Identifying unique classes across the dataset...\")\n",
    "    all_data = pd.read_csv('data/preprocessed_microsoft365_user_activity.csv', usecols=['deviceCategory'])\n",
    "    unique_classes = np.unique(all_data['deviceCategory'].astype(np.float64))\n",
    "    del all_data\n",
    "    gc.collect()\n",
    "    print(f\"Unique classes identified: {unique_classes}\")\n",
    "\n",
    "    # Initialize model with reduced complexity\n",
    "    model = SGDClassifier(\n",
    "        loss='log_loss',  # Use 'log_loss' for classification\n",
    "        penalty='l2',\n",
    "        alpha=0.001,\n",
    "        max_iter=1,  # Enable partial_fit\n",
    "        warm_start=True,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Process data in chunks and train incrementally\n",
    "    print(\"\\nProcessing data and training model incrementally...\")\n",
    "    chunks = pd.read_csv('data/preprocessed_microsoft365_user_activity.csv', chunksize=chunk_size)\n",
    "\n",
    "    accumulated_predictions = []\n",
    "    accumulated_true_values = []\n",
    "\n",
    "    for chunk_num, chunk in enumerate(tqdm(chunks)):\n",
    "        chunk, label_encoders = process_chunk(chunk, label_encoders, first_chunk)\n",
    "        if first_chunk:\n",
    "            feature_names = chunk.columns.drop('deviceCategory').tolist()\n",
    "            first_chunk = False\n",
    "\n",
    "        X_chunk = chunk.drop('deviceCategory', axis=1)\n",
    "        y_chunk = chunk['deviceCategory'].values\n",
    "\n",
    "        # Perform incremental training\n",
    "        model.partial_fit(X_chunk, y_chunk, classes=unique_classes)\n",
    "\n",
    "        # Evaluate on every 5th chunk\n",
    "        if chunk_num % 5 == 0:\n",
    "            y_pred = model.predict(X_chunk)\n",
    "            accumulated_predictions.extend(y_pred)\n",
    "            accumulated_true_values.extend(y_chunk)\n",
    "\n",
    "        del chunk, X_chunk, y_chunk\n",
    "        gc.collect()\n",
    "\n",
    "    # Evaluate model\n",
    "    print(\"\\nEvaluating model...\")\n",
    "    print(\"\\nClassification Report:\\n\", \n",
    "          classification_report(accumulated_true_values, accumulated_predictions))\n",
    "    print(\"\\nConfusion Matrix:\\n\", \n",
    "          confusion_matrix(accumulated_true_values, accumulated_predictions))\n",
    "    print(\"\\nAccuracy Score:\", \n",
    "          accuracy_score(accumulated_true_values, accumulated_predictions))\n",
    "\n",
    "    # Save results\n",
    "    print(\"\\nSaving results...\")\n",
    "    results_df = pd.DataFrame({\n",
    "        'Actual': accumulated_true_values,\n",
    "        'Predicted': accumulated_predictions\n",
    "    })\n",
    "    results_df.to_csv('data/model_results.csv', index=False)\n",
    "\n",
    "    # Save model\n",
    "    import joblib\n",
    "    joblib.dump(model, 'models/sgd_classifier_model.joblib')\n",
    "\n",
    "    print(\"\\nAnalysis completed!\")\n",
    "    display_memory_usage()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    import traceback\n",
    "    print(traceback.format_exc())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis...\n",
      "Current memory usage: 224.66 MB\n",
      "Identifying unique classes across the dataset...\n",
      "Unique classes identified: [0. 1. 2.]\n",
      "\n",
      "Processing data and training model incrementally...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "749it [01:27,  8.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model...\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.86      0.85    113150\n",
      "         1.0       0.49      0.40      0.44     32039\n",
      "         2.0       0.06      0.10      0.07      4811\n",
      "\n",
      "    accuracy                           0.74    150000\n",
      "   macro avg       0.46      0.45      0.45    150000\n",
      "weighted avg       0.74      0.74      0.74    150000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[97221 11508  4421]\n",
      " [16162 12760  3117]\n",
      " [ 2617  1732   462]]\n",
      "\n",
      "Accuracy Score: 0.7362866666666666\n",
      "\n",
      "Saving results...\n",
      "\n",
      "Analysis completed!\n",
      "Current memory usage: 234.38 MB\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
